{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchmetrics\n",
    "\n",
    "from datasets import load_dataset, get_dataset_split_names\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import math\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_module: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_module = d_module\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_length: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.size(1), :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 10**-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, d_k, mask, dropout: nn.Dropout=None):\n",
    "        d_k = query.shape[-1]\n",
    "        \n",
    "        # (batch_size, h, seq_length, d_k) --> (batch_size, h, seq_length, seq_length)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1) # (batch_size, h, seq_length, seq_length)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        # (batch_size, h, seq_length, seq_length) --> (batch_size, h, seq_length, d_k)\n",
    "        attention_output = torch.matmul(attention_scores, value)\n",
    "        return (attention_output) , attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.W_Q(q)  # (batch_size, seq_length, d_model) --> (batch_size, seq_length, d_model)\n",
    "        key = self.W_K(k)   # (batch_size, seq_length, d_model) --> (batch_size, seq_length, d_model)\n",
    "        value = self.W_V(v) # (batch_size, seq_length, d_model) --> (batch_size, seq_length, d_model)\n",
    "\n",
    "        # (batch_size, seq_length, d_model) --> (batch_size, h, seq_length, d_k) --> (batch_size, h, seq_length, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # (batch_size, h, seq_length, d_k) --> (batch_size, h, seq_length, d_k)\n",
    "        x, self.attention_scores = self.attention(query, key, value, self.d_k, mask, self.dropout)\n",
    "\n",
    "        # (batch_size, h, seq_length, d_k) --> (batch_size, seq_length, h, d_k) --> (batch_size, seq_length, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)  # self.h * self.d_k = d_model\n",
    "\n",
    "        # (batch_size, seq_length, d_model) --> (batch_size, seq_length, d_model)\n",
    "        return self.W_O(x)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.layer_norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection1 = ResidualConnection(dropout)\n",
    "        self.residual_connection2 = ResidualConnection(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connection1(x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connection2(x, self.feed_forward_block)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock,\n",
    "                 feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection1 = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connection1[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connection1[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connection1[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_length, d_model) --> (batch_size, seq_length, vocab_size)\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbedding, tgt_embed: InputEmbedding,\n",
    "                 src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(vocab_size: int, src_seq_len: int, tgt_seq_len: int,\n",
    "                      d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    src_embed = InputEmbedding(d_model, vocab_size)\n",
    "    tgt_embed = InputEmbedding(d_model, vocab_size)\n",
    "\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    encoder_blocks = []\n",
    "\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_blocks.append(EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout))\n",
    "\n",
    "    decoder_blocks = []\n",
    "\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    projection_layer = ProjectionLayer(d_model, vocab_size)\n",
    "\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eli5Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer, src_seq_len, tgt_seq_len):\n",
    "        super().__init__()\n",
    "        self.src_seq_len = src_seq_len\n",
    "        self.tgt_seq_len = tgt_seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.ds[idx][\"title\"]\n",
    "        answers = self.ds[idx][\"answers\"][\"text\"]\n",
    "        \n",
    "        # write torch random function to choose an idx from answers\n",
    "        answer_idx = np.random.randint(0, len(answers))\n",
    "        answer = answers[answer_idx]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer.encode(title).ids\n",
    "        dec_input_tokens = self.tokenizer.encode(answer).ids\n",
    "\n",
    "        if len(dec_input_tokens) > self.tgt_seq_len - 1:\n",
    "            num_tokens_list = []\n",
    "            for i, answer in enumerate(answers):\n",
    "                num_tokens_list.append(len(self.tokenizer.encode(answer).ids))\n",
    "\n",
    "            answer_idx = np.argmax(num_tokens_list)\n",
    "            answer = answers[answer_idx]\n",
    "            dec_input_tokens = self.tokenizer.encode(answer).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.src_seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.tgt_seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0:\n",
    "            # raise ValueError(\"Sentence is too long\")\n",
    "            enc_input_tokens = enc_input_tokens[:self.src_seq_len - 2]\n",
    "            enc_num_padding_tokens = 0\n",
    "\n",
    "        if dec_num_padding_tokens < 0:\n",
    "            # raise ValueError(\"Sentence is too long\")\n",
    "            dec_input_tokens = dec_input_tokens[:self.tgt_seq_len - 1]\n",
    "            dec_num_padding_tokens = 0\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.src_seq_len\n",
    "        assert decoder_input.size(0) == self.tgt_seq_len\n",
    "        assert label.size(0) == self.tgt_seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": title,\n",
    "            \"tgt_text\": answer if len(answer.split()) < self.tgt_seq_len else answer[:self.tgt_seq_len]\n",
    "        }\n",
    "    \n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds):\n",
    "    for item in tqdm(ds):\n",
    "        yield item['title']\n",
    "        for sentence in item[\"answers\"]['text']:\n",
    "            yield sentence\n",
    "\n",
    "def get_or_build_tokenizer(config, ds):\n",
    "    tokenizer_path = Path(config['tokenizer_file'])\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        print(f\"Loading tokenizer from file {tokenizer_path}\")\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "def get_ds_and_tokenizer(config):\n",
    "    print('Loading dataset')\n",
    "    ds_train_raw = load_dataset(\"eli5\", split=\"train_eli5\")\n",
    "    ds_val_raw = load_dataset(\"eli5\", split=\"validation_eli5\")\n",
    "    ds_test_raw = load_dataset(\"eli5\", split=\"test_eli5\")\n",
    "    print('Dataset loaded')\n",
    "\n",
    "    print('Building tokenizer')\n",
    "    tokenizer = get_or_build_tokenizer(config, ds_train_raw)\n",
    "    print('Tokenizer built')\n",
    "\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    # print('Finding max length of source and target sentences')\n",
    "    # for item in tqdm(ds_train_raw):\n",
    "    #     src_ids = tokenizer.encode(item['title']).ids\n",
    "    #     max_len_src = max(max_len_src, len(src_ids))\n",
    "\n",
    "    #     for sentence in item[\"answers\"][\"text\"]:\n",
    "    #         tgt_ids = tokenizer.encode(sentence).ids\n",
    "    #         max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    # print(f'Max length of source sentence: {max_len_src}')\n",
    "    # print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "    if max_len_src > config['src_seq_len']:\n",
    "        print(f'Warning: max length of source sentence is greater than the config value of {config[\"src_seq_len\"]}')\n",
    "    if max_len_tgt > config['tgt_seq_len']:\n",
    "        print(f'Warning: max length of target sentence is greater than the config value of {config[\"tgt_seq_len\"]}')\n",
    "\n",
    "    train_ds = Eli5Dataset(ds_train_raw, tokenizer, config['src_seq_len'], config['tgt_seq_len'])\n",
    "    val_ds = Eli5Dataset(ds_val_raw, tokenizer, config['src_seq_len'], config['tgt_seq_len'])\n",
    "    test_ds = Eli5Dataset(ds_test_raw, tokenizer, config['src_seq_len'], config['tgt_seq_len'])\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True, num_workers=2)\n",
    "    test_dataloader = DataLoader(test_ds, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prev_state(config, model, optimizer, initial_epoch, global_step):\n",
    "    \n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "    return model, optimizer, initial_epoch, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer, max_dec_seq_len, device):\n",
    "    sos_idx = tokenizer.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_dec_seq_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "def run_validation(model, validation_ds, tokenizer, max_dec_seq_len, device, print_msg, global_step, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "\n",
    "    console_width = 20\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            print(f\"Validation example: {count}\")\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer, max_dec_seq_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            \n",
    "            # print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        # Compute the char error rate \n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(config, model, optimizer, loss_fn, train_dataloader, val_dataloader, test_loader, tokenizer, device):\n",
    "    model, optimizer, initial_epoch, global_step = load_prev_state(config, model, optimizer, 0, 0)\n",
    "\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "\n",
    "        \n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer, config['tgt_seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer, 2)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"batch_size\": 16,\n",
    "        \"num_epochs\": 8,\n",
    "        \"lr\": 10**-4,\n",
    "        'src_seq_len': 128,\n",
    "        'tgt_seq_len': 256,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'eli5',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "if (device == 'cuda'):\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "    print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "device = torch.device(device)\n",
    "\n",
    "Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "train_dataloader, val_dataloader, test_loader, tokenizer = get_ds_and_tokenizer(config)\n",
    "\n",
    "model = build_transformer(tokenizer.get_vocab_size(), config[\"src_seq_len\"], config['tgt_seq_len'], d_model=config['d_model']).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce RTX 3090\n",
      "Device memory: 23.68316650390625 GB\n",
      "Loading dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n",
      "Building tokenizer\n",
      "Loading tokenizer from file tokenizer.json\n",
      "Tokenizer built\n",
      "No model to preload, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 17040/17040 [39:18<00:00,  7.23it/s, loss=5.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation example: 1\n",
      "--------------------\n",
      "    SOURCE: My Daughter asked me what condensation is, I tried I explaining but she didn't understand. I need a genuine ELI5 on what condensation is.\n",
      "    TARGET: You can find water in the air. It's called humidity or moisture.\n",
      "\n",
      "**\"BUT WATER IS A LIQUID!!\"** Kinda, water prefers to be in the liquid state but a small percentage goes to the air, this percentage depends mostly on the temperature. Hot environment can hold more water in the air and Cold ones don't hold up to as much water. \n",
      "\n",
      "**Are you serious?! This happens?**\" Yes. Actually how to clothes dry up in the sun? The shirts don't heat up to 100ºC / 212ºF. The sun just heats up the air around the shirt and the water mixes up with the air. Removing it from the shirt to the air.\n",
      "\n",
      "**WHAT HAS THIS TO DO WITH CONDENSATION?!\"** Well, hot environment hold more water than cold ones. So what happens when he pass from hot air to cold air? Well, the cold air can't hold up as much water and the water starts to leave the air and forms droplets.\n",
      "\n",
      "Usually this happens in walls or cold surfaces like Mirrors or windows because the hot air is touching there and getting colder. \n",
      "\n",
      "Any doubts or further explanation PM me, I'm a chem engineer so I'm at ease with the matter.\n",
      "\n",
      "EDIT: Forgot the \"r\" in shirt.......... sorry\n",
      " PREDICTED: The water is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic , and is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , which is a diuretic , and is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , and is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , and is a diuretic . The water is a diuretic , which is a diuretic . The water is a diuretic , and the water is a diuretic . The water is a diuretic ,\n",
      "Validation example: 2\n",
      "--------------------\n",
      "    SOURCE: Why isn't the Westboro Baptist Church considered a hate group in the U.S. and has all the legal benefits of a religion?\n",
      "    TARGET: In the US, \"hate groups\" are legal.  You can espouse hate all you want, because you have freedom of speech.  You just can't threaten people, or attempt to incite others to violence.\n",
      " PREDICTED: The US is a very good idea of how the US is . The US is a very good example of the US , and the US is a very good idea of how the US is . The US is a very good example of the US , and the US is a very good idea . The US is a very good example of the US , and the US is a very good idea . The US is a very good example of the US , and the US is a good idea of how the US is . The US is a very good example of the US , and the US is a very good idea of how the US is . The US is a very good example of the US , and the US is a very good example of the US . The US is a very good example of the US , and the US is a lot of the US . The US is a very good example of the US , and the US is a lot of the US . The US is a very good example of the US , and the US is a lot of the US . The US is a very good example of the US , and the US is a lot of the US . The US is a lot of the US government , and the US is a lot of the US . The\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01:   5%|▍         | 800/17040 [01:51<37:34,  7.20it/s, loss=5.462]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(config)\n",
      "Cell \u001b[0;32mIn[42], line 80\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     77\u001b[0m writer\u001b[39m.\u001b[39mflush()\n\u001b[1;32m     79\u001b[0m \u001b[39m# Backpropagate the loss\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     82\u001b[0m \u001b[39m# Update the weights\u001b[39;00m\n\u001b[1;32m     83\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(config, model, optimizer, loss_fn, train_dataloader, val_dataloader, test_loader, tokenizer, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
