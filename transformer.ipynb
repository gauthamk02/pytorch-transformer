{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchmetrics\n",
    "\n",
    "from datasets import load_dataset, get_dataset_split_names\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import math\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_module: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_module = d_module\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_length: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.size(1), :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 10**-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, d_k, mask, dropout: nn.Dropout=None):\n",
    "        d_k = query.shape[-1]\n",
    "        \n",
    "        # (batch_size, h, seq_length, d_k) --> (batch_size, h, seq_length, seq_length)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1) # (batch_size, h, seq_length, seq_length)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        # (batch_size, h, seq_length, seq_length) --> (batch_size, h, seq_length, d_k)\n",
    "        attention_output = torch.matmul(attention_scores, value)\n",
    "        return (attention_output) , attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.W_Q(q)  # (batch_size, seq_length, d_model) --> (batch_size, seq_length, d_model)\n",
    "        key = self.W_K(k)   # (batch_size, seq_length, d_model) --> (batch_size, seq_length, d_model)\n",
    "        value = self.W_V(v) # (batch_size, seq_length, d_model) --> (batch_size, seq_length, d_model)\n",
    "\n",
    "        # (batch_size, seq_length, d_model) --> (batch_size, h, seq_length, d_k) --> (batch_size, h, seq_length, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # (batch_size, h, seq_length, d_k) --> (batch_size, h, seq_length, d_k)\n",
    "        x, self.attention_scores = self.attention(query, key, value, self.d_k, mask, self.dropout)\n",
    "\n",
    "        # (batch_size, h, seq_length, d_k) --> (batch_size, seq_length, h, d_k) --> (batch_size, seq_length, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)  # self.h * self.d_k = d_model\n",
    "\n",
    "        # (batch_size, seq_length, d_model) --> (batch_size, seq_length, d_model)\n",
    "        return self.W_O(x)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.layer_norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection1 = ResidualConnection(dropout)\n",
    "        self.residual_connection2 = ResidualConnection(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connection1(x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connection2(x, self.feed_forward_block)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock,\n",
    "                 feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection1 = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connection1[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connection1[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connection1[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_length, d_model) --> (batch_size, seq_length, vocab_size)\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbedding, tgt_embed: InputEmbedding,\n",
    "                 src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, seq_len: int,\n",
    "                      d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    src_embed = InputEmbedding(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbedding(d_model, tgt_vocab_size)\n",
    "\n",
    "    src_pos = PositionalEncoding(d_model, seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, seq_len, dropout)\n",
    "\n",
    "    encoder_blocks = []\n",
    "\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_blocks.append(EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout))\n",
    "\n",
    "    decoder_blocks = []\n",
    "\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "    \n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "def get_ds_and_tokenizer(config):\n",
    "    print('Loading dataset and Tokenizer')\n",
    "    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    print('Dataset and Tokenizer loaded')\n",
    "\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    print('Finding max length of source and target sentences')\n",
    "    for item in tqdm(ds_raw):\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "   \n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prev_state(config, model, optimizer, initial_epoch, global_step):\n",
    "    \n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "    return model, optimizer, initial_epoch, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    console_width = 60\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    metric = torchmetrics.CharErrorRate()\n",
    "    cer = metric(predicted, expected)\n",
    "    \n",
    "    metric = torchmetrics.WordErrorRate()\n",
    "    wer = metric(predicted, expected)\n",
    "\n",
    "    metric = torchmetrics.BLEUScore()\n",
    "    bleu = metric(predicted, expected)\n",
    "\n",
    "    print_msg(f'Char Error Rate: {cer:.2f}\\t Word Error Rate: {wer:.2f}\\t BLEU: {bleu:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(config, model, optimizer, loss_fn, train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt, device):\n",
    "    model, optimizer, initial_epoch, global_step = load_prev_state(config, model, optimizer, 0, 0)\n",
    "\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "    \n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "    \n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg))\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "        print(f\"Saved model to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce RTX 3090\n",
      "Device memory: 23.68316650390625 GB\n",
      "Loading dataset and Tokenizer\n",
      "Dataset and Tokenizer loaded\n",
      "Finding max length of source and target sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32332/32332 [00:03<00:00, 10412.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "if (device == 'cuda'):\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "    print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "device = torch.device(device)\n",
    "\n",
    "Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds_and_tokenizer(config)\n",
    "\n",
    "model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(), config['seq_len'], config['d_model']).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model opus_books_weights/tmodel_00.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|██████████| 3638/3638 [07:29<00:00,  8.09it/s, loss=4.991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: 'Why, he's jealous!' she thought. 'Oh dear!\n",
      "    TARGET: “Perché è geloso — ella pensava. — Dio mio! com’è simpatico e sciocco!\n",
      " PREDICTED: — E che cosa è accaduto — disse lei . — E non è accaduto .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: He had a talent for understanding art and for imitating it with accuracy and good taste, and he imagined that he possessed the real power an artist needs.\n",
      "    TARGET: Aveva attitudine a intendere l’arte e ad imitare con fedeltà, con gusto, l’opera d’arte; credette così d’avere ciò che occorre all’artista.\n",
      " PREDICTED: Era un ’ altra cosa , e , e , e , e , e , e , e , e , e , per la sua vita , e la sua vita .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.73\t Word Error Rate: 1.38\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_01.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████| 3638/3638 [07:30<00:00,  8.08it/s, loss=4.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: \"I hope we are friends,\" was the unmoved reply; while he still watched the rising of the moon, which he had been contemplating as I approached.\n",
      "    TARGET: — Spero invece che noi siamo amici, — mi disse fissando la luna.\n",
      " PREDICTED: — Io sono — rispose il signor Rochester , — che mi la luna , e che mi di .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: \"Did you see her face?\"\n",
      "    TARGET: — Avete veduto la sua fisonomia?\n",
      " PREDICTED: — Avete sentito il viso ?\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.74\t Word Error Rate: 1.11\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_02.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████| 3638/3638 [07:30<00:00,  8.07it/s, loss=4.814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: At three she also left, promising to come back to dinner.\n",
      "    TARGET: Alle tre se ne andò anche lei, promettendo di venire a pranzo.\n",
      " PREDICTED: In tre volte , dopo , si mise a la mattina .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: They'll water the horses at the wrong time, tear good harness, change a wheel with an iron tire for one without, or drop a bolt into the threshing machine in order to break it.\n",
      "    TARGET: Abbevera i cavalli così da farli scoppiare, una bardatura buona la rompe, una ruota cerchiata ve la cambia e se la beve; nella macchina per la battitura ci getta un perno, per spezzarla.\n",
      " PREDICTED: a il bosco , il quale , , un ’ altra parte , senza , o un , senza , senza , né un .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.73\t Word Error Rate: 0.96\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_03.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████| 3638/3638 [07:30<00:00,  8.07it/s, loss=4.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: It came very warmly upon my thoughts, and indeed irresistibly, that now was the time to get me a servant, and, perhaps, a companion or assistant; and that I was plainly called by Providence to save this poor creature’s life.\n",
      "    TARGET: Adesso sì che mi tornava caldamente e in guisa invincibile la mia prediletta idea di procacciarmi un servo e forse un compagno o aiutante; adesso sì, diceva a me stesso, che ne è arrivato il tempo; adesso sono l’uomo chiamato dalla Provvidenza a salvare la vita di quella povera creatura.\n",
      " PREDICTED: Mi fece un po ’ di gratitudine e di , perchè la mia storia era stata una volta , mi fu un pezzo di cui mi aveva fatto , e mi diedi a la mia vita , e mi diedi a la vita .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: 'Yes, all my hopes are fixed on you,' said her brother.\n",
      "    TARGET: — Sì, ogni speranza è in te — disse Stepan Arkad’ic.\n",
      " PREDICTED: — Sì , tutto il mio arrivo , voi avete sentito — disse lei .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.72\t Word Error Rate: 0.92\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_04.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05: 100%|██████████| 3638/3638 [07:31<00:00,  8.06it/s, loss=4.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: Whether is it better, I ask, to be a slave in a fool's paradise at Marseilles--fevered with delusive bliss one hour--suffocating with the bitterest tears of remorse and shame the next--or to be a village-schoolmistress, free and honest, in a breezy mountain nook in the healthy heart of England?\n",
      "    TARGET: Che cosa era meglio, domando: vivere schiava in un paradiso d'amore, trascinata un momento nel vortice di una felicità, e soffocata dopo subito dalle lagrime amare del rimorso e della vergogna, o esser maestra libera e onorata in un villaggio, fra le montagne dell'Inghilterra?\n",
      " PREDICTED: \" Se è meglio , vi un ' altra donna , un ' altra donna , con una donna che fa , con la sua bontà e la sua bontà , la sua bontà , la sua bontà , la quale ha un ' altra donna , la testa di quelle colline ?\n",
      "------------------------------------------------------------\n",
      "    SOURCE: 'You could go there to-morrow!' she said.\n",
      "    TARGET: — Puoi andare domani — disse.\n",
      " PREDICTED: — Tu non andare a domani — disse lei .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.66\t Word Error Rate: 1.14\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_05.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 06: 100%|██████████| 3638/3638 [07:29<00:00,  8.10it/s, loss=4.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: \"No; you shall tear yourself away, none shall help you: you shall yourself pluck out your right eye; yourself cut off your right hand: your heart shall be the victim, and you the priest to transfix it.\"\n",
      "    TARGET: — Ti sbranerai da te, e nessuno ti aiuterà; ti strapperai l'occhio, ti strapperai la mano diritta; il cuore sarà la vittima e tu il carnefice.\n",
      " PREDICTED: — No , non vi , non vi , non vi , il cuore , e vi il cuore , e vi il cuore .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: \"Yes, sir; in different ways, I have an affection for both.\"\n",
      "    TARGET: — Sì, signore, voglio bene a tutte e due, benché in modo differente.\n",
      " PREDICTED: — Sì , signore , e in uno stato di quelli che ho amici e di cuore .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.64\t Word Error Rate: 1.00\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_06.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 07: 100%|██████████| 3638/3638 [07:29<00:00,  8.09it/s, loss=4.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: \"Gratitude!\" he ejaculated; and added wildly--\"Jane accept me quickly. Say, Edward--give me my name--Edward--I will marry you.\" \"Are you in earnest? Do you truly love me?\n",
      "    TARGET: — Gratitudine! — esclamò; ed aggiunse violentemente: — Jane, accettatemi subito, chiamatemi per nome, ditemi: Edoardo, Edoardo, vi voglio sposare.\n",
      " PREDICTED: — ! — continuò , — , — fate il mio ; volete che mi , Jane , vi , vi , vi .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: The old man accomplished this with ease.\n",
      "    TARGET: Il vecchio lo faceva con facilità.\n",
      " PREDICTED: Il vecchio , che si , si sempre .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.69\t Word Error Rate: 1.04\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_07.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 08: 100%|██████████| 3638/3638 [07:29<00:00,  8.10it/s, loss=3.002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: I hope this delay will not have increased the difficulty of securing it.\"\n",
      "    TARGET: Spero che questo ritardo non avrà resa più difficile l'ottenerla.\n",
      " PREDICTED: Spero che non nulla di tempo a lavorare , perché la difficoltà dell ' atto di .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: Harris said I encouraged him.\n",
      "    TARGET: Harris disse che ero io che lo incoraggiavo.\n",
      " PREDICTED: Harris gli dissi che gli risposi .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.66\t Word Error Rate: 1.22\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_08.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 09: 100%|██████████| 3638/3638 [07:29<00:00,  8.09it/s, loss=3.813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: 'No, I don't.\n",
      "    TARGET: — No, non lo conosco.\n",
      " PREDICTED: — No , non lo farò .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: \"Go back and fetch both.\"\n",
      "    TARGET: — Andate a prendere queste due cose.\n",
      " PREDICTED: — Andate a prendere e a prendere due .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.37\t Word Error Rate: 0.67\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_09.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 10: 100%|██████████| 3638/3638 [07:30<00:00,  8.08it/s, loss=3.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: CHAPTER VIII\n",
      "    TARGET: VIII.\n",
      " PREDICTED: VIII\n",
      "------------------------------------------------------------\n",
      "    SOURCE: \"She is in Miss Temple's room,\" said the nurse.\n",
      "    TARGET: — In quella della signorina Temple, — mi rispose l'infermiera.\n",
      " PREDICTED: — È nella camera della direttrice , — disse la njanja .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.61\t Word Error Rate: 0.91\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 11: 100%|██████████| 3638/3638 [07:30<00:00,  8.07it/s, loss=2.892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: He knew the difficulties connected with such a step: but he had said he would do it and was now obliged to do it.\n",
      "    TARGET: Egli conosceva tutte le difficoltà collegate a questa faccenda, ma aveva detto che lo avrebbe fatto ed ora doveva mettere in atto la minaccia.\n",
      " PREDICTED: Egli sapeva che le difficoltà sarebbero finite di un passo così , ma ora egli avrebbe voluto .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: Poor Alice!\n",
      "    TARGET: Povera Alice!\n",
      " PREDICTED: Alice !\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.63\t Word Error Rate: 0.85\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 12: 100%|██████████| 3638/3638 [07:30<00:00,  8.08it/s, loss=2.596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: 'It happens that I am expecting visitors,' replied Levin more rapidly, breaking off the splintered bits of the stick with his strong fingers. 'Or no, I am not expecting visitors and nothing has happened, yet I request you to leave.\n",
      "    TARGET: — È accaduto che aspetto ospiti — disse Levin, rompendo sempre più in fretta con le dita forti le estremità del bastone che s’era spaccato. — Anzi, non aspetto ospiti e non è accaduto nulla, ma vi prego di partire.\n",
      " PREDICTED: — È questo che vado — rispose Levin , sempre più il punto che , il bastone sopra delle sinistra . — O , forse non abbiamo neppure intenzione di andare a chiedere se n ’ è andato , ma son andato via di nuovo a chiedere aiuto .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: Her head dropped.\n",
      "    TARGET: Ella chinò il capo.\n",
      " PREDICTED: Il capo si lasciò .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.73\t Word Error Rate: 1.05\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 13: 100%|██████████| 3638/3638 [07:30<00:00,  8.07it/s, loss=3.004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: The man, the human being, broke the spell at once. Nothing ever rode the Gytrash: it was always alone; and goblins, to my notions, though they might tenant the dumb carcasses of beasts, could scarce covet shelter in the commonplace human form.\n",
      "    TARGET: La vista dell'uomo sfatò l'incantesimo, perché nessun essere umano aveva mai cavalcato Gytrash.\n",
      " PREDICTED: L ' uomo era umano , l ' aspetto umano si vedeva . Non era mai veduto il mio spirito , e mi sentivo sempre triste , perché era ben sicuro di scendere in mezzo ai miei simili modi , ma potevano esser liberato .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: (Aside to pianist): \"It is too low, old man; we'll have that over again, if you don't mind.\"\n",
      "    TARGET: (Da parte al pianista): — È troppo basso, caro; ricominceremo da capo se non vi dispiace.\n",
      " PREDICTED: ( allegramente ): — Fa troppo forte , caro ; facciamo il tuo bel giuramento .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 1.12\t Word Error Rate: 2.00\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_13.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 14: 100%|██████████| 3638/3638 [07:30<00:00,  8.08it/s, loss=2.831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: And her clothes, they will wear out: how can she get new ones?\"\n",
      "    TARGET: E poi i suoi vestiti si consumeranno, e come farà ad averne altri?\n",
      " PREDICTED: E le si , le si devono esser forse più vicino a lei ?\n",
      "------------------------------------------------------------\n",
      "    SOURCE: While he is so occupied, I will tell you, reader, what they are: and first, I must premise that they are nothing wonderful.\n",
      "    TARGET: Mentre che il signor Rochester li considerava ho il tempo di descriverli.\n",
      " PREDICTED: Mentre egli si allontana da lui , vi dirò come siano ; e appunto in che cosa sono , devo far nulla .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.80\t Word Error Rate: 1.36\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_14.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 15: 100%|██████████| 3638/3638 [07:31<00:00,  8.06it/s, loss=2.817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: She went up to them, talked to them, and acted as interpreter for the woman, who spoke nothing but Russian.\n",
      "    TARGET: Si avvicinava loro, conversava, faceva da interprete alla donna che non parlava nessuna lingua straniera.\n",
      " PREDICTED: Ella si avvicinò a loro , le esaminò i , e gli parve poco la donna che non esprimeva nulla .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: The third prod did it: and he turned over on the other side, and said he would be down in a minute, and that he would have his lace-up boots.\n",
      "    TARGET: Il terzo colpo fece effetto; ma Harris si voltò sull’altro lato, dicendo che si sarebbe levato in un minuto e che si sarebbe subito infilati gli stivaletti.\n",
      " PREDICTED: L ’ ultimo segno si fermò ; poi , sorridendo su di un fianco , dicendo : si sarebbe seduto su di un monticello e gli scarpe .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.62\t Word Error Rate: 1.10\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_15.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 16: 100%|██████████| 3638/3638 [07:30<00:00,  8.08it/s, loss=2.466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: You have heard of her misfortune?\n",
      "    TARGET: Sapete la sua sventura?\n",
      " PREDICTED: Avete inteso la sua sventura ?\n",
      "------------------------------------------------------------\n",
      "    SOURCE: 'But we are not talking about that.'\n",
      "    TARGET: — Ma noi non parliamo di questo.\n",
      " PREDICTED: — Ma noi non parli più con questo .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.33\t Word Error Rate: 0.82\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_16.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 17: 100%|██████████| 3638/3638 [07:30<00:00,  8.07it/s, loss=2.520]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: Now I looked back upon my desolate, solitary island as the most pleasant place in the world and all the happiness my heart could wish for was to be but there again. I stretched out my hands to it, with eager wishes—“O happy desert!” said I, “I shall never see thee more.\n",
      "    TARGET: Ora io m’augurava la mia desolata e solitaria isola come se fosse il più delizioso paese dell’universo; ora tutta la felicità che il mio cuore sapesse desiderare, era il tornare ad esservi di bel nuovo; stendeva sospirando le mani verso di essa: «Oh fortunato deserto! io esclamava, non ti vedrò mai più! Misera creatura ch’io sono!\n",
      " PREDICTED: Ora mi fermai , o la mia vita errante , come la più bella e il mondo si rallegrò pienamente e il cuore di essere in me solo per essere in me . Ma io non potevo persuadermi che il cuore fosse rimasto in me , mi più e ad ogni felicità più , la piena di vita .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: 'It's the thing Mock Turtle Soup is made from,' said the Queen.\n",
      "    TARGET: — È quella con cui si fa la minestra di Falsa-testuggine, — disse la Regina.\n",
      " PREDICTED: — È la canzone della seguente della Regina .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.65\t Word Error Rate: 0.94\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_17.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 18: 100%|██████████| 3638/3638 [07:30<00:00,  8.08it/s, loss=2.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: \"And Miss Ingram: what sort of a voice had she?\"\n",
      "    TARGET: — E che voce ha la signorina Ingram?\n",
      " PREDICTED: — E quella signorina Ingram ?\n",
      "------------------------------------------------------------\n",
      "    SOURCE: Why, not even from the members of his own family did he receive what you could call active encouragement.\n",
      "    TARGET: Ebbene, neppure dai membri della propria famiglia ricevè ciò che si chiamerebbe un attivo incoraggiamento.\n",
      " PREDICTED: Perché non solo dei membri della famiglia , egli si occupava di quello che egli supponeva , avrebbe saputo delle terre .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.64\t Word Error Rate: 1.09\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_18.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 19: 100%|██████████| 3638/3638 [07:30<00:00,  8.07it/s, loss=2.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "    SOURCE: The room Levin entered was a large one with a tiled stove and a partition.\n",
      "    TARGET: La stanza era grande, con una stufa olandese e un’intelaiatura.\n",
      " PREDICTED: La sala da pranzo era in una grande e morbide , da mangiare , da un tramezzo .\n",
      "------------------------------------------------------------\n",
      "    SOURCE: On the day when, in the ballroom of the house in Arbat Street, she in her brown dress had gone up to him and silently plighted herself to him, on that day and in that hour a complete rupture seemed to have taken place within her soul between her former life and this other new and entirely unknown life – although in fact the old life still went on.\n",
      "    TARGET: Nell’animo suo, in quel giorno in cui, in abito marrone, nella sala della casa sull’Arbat, si era avvicinata a lui in silenzio e gli si era data, nell’animo suo, in quel giorno e in quell’ora si era compiuto un completo distacco da tutta la sua vita di prima ed era cominciata, pur continuando in realtà l’antica, un’altra vita, completamente nuova, completamente sconosciuta.\n",
      " PREDICTED: Nel giorno in cui , nel ballo in cui aveva scritto , ella aveva già indossato i abiti del vestito preferito di saltare , giaceva in silenzio e si sprofondò in quell ’ ora nel suo complicato libro , che da un momento oscuro , che sembrava ancor più di più e perfino di nuovo , sebbene questo fosse sempre più nuovo , e ancora nuovo , malgrado la vita .\n",
      "------------------------------------------------------------\n",
      "Char Error Rate: 0.70\t Word Error Rate: 1.12\t BLEU: 0.00\n",
      "Saved model to opus_books_weights/tmodel_19.pt\n"
     ]
    }
   ],
   "source": [
    "train_model(config, model, optimizer, loss_fn, train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
